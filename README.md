# BioBART
BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model

# Model Checkpoints

- Base Version (6 + 6 Layers): GanjinZero/biobart-base
- Large Version (12 + 12 Layers): GanjinZero/biobart-large

Two line usages:
```python
model = AutoModel.from_pretrained('GanjinZero/biobart-base')
tok = AutoTokenizer.from_pretrained('GanjinZero/biobart-base')
```

# Citation
TBA.
<!-- Tsinghua University \& International Digital Economy Academy. -->
